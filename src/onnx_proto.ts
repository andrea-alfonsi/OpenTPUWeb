// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.2
//   protoc               v3.12.4
// source: onnx.proto3

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";

export const protobufPackage = "onnx";

/**
 * Versioning
 *
 * ONNX versioning is specified in docs/IR.md and elaborated on in docs/Versioning.md
 *
 * To be compatible with both proto2 and proto3, we will use a version number
 * that is not defined by the default value but an explicit enum number.
 */
export enum Version {
  /**
   * _START_VERSION - proto3 requires the first enum value to be zero.
   * We add this just to appease the compiler.
   */
  _START_VERSION = 0,
  /**
   * IR_VERSION_2017_10_10 - The version field is always serialized and we will use it to store the
   * version that the  graph is generated from. This helps us set up version
   * control.
   * For the IR, we are using simple numbers starting with 0x00000001,
   * which was the version we published on Oct 10, 2017.
   */
  IR_VERSION_2017_10_10 = 1,
  /**
   * IR_VERSION_2017_10_30 - IR_VERSION 2 published on Oct 30, 2017
   * - Added type discriminator to AttributeProto to support proto3 users
   */
  IR_VERSION_2017_10_30 = 2,
  /**
   * IR_VERSION_2017_11_3 - IR VERSION 3 published on Nov 3, 2017
   * - For operator versioning:
   *    - Added new message OperatorSetIdProto
   *    - Added opset_import in ModelProto
   * - For vendor extensions, added domain in NodeProto
   */
  IR_VERSION_2017_11_3 = 3,
  /**
   * IR_VERSION_2019_1_22 - IR VERSION 4 published on Jan 22, 2019
   * - Relax constraint that initializers should be a subset of graph inputs
   * - Add type BFLOAT16
   */
  IR_VERSION_2019_1_22 = 4,
  /**
   * IR_VERSION_2019_3_18 - IR VERSION 5 published on March 18, 2019
   * - Add message TensorAnnotation.
   * - Add quantization annotation in GraphProto to map tensor with its scale and zero point quantization parameters.
   */
  IR_VERSION_2019_3_18 = 5,
  /**
   * IR_VERSION_2019_9_19 - IR VERSION 6 published on Sep 19, 2019
   * - Add support for sparse tensor constants stored in model.
   *   - Add message SparseTensorProto
   *   - Add sparse initializers
   */
  IR_VERSION_2019_9_19 = 6,
  /**
   * IR_VERSION_2020_5_8 - IR VERSION 7 published on May 8, 2020
   * - Add support to allow function body graph to rely on multiple external opreator sets.
   * - Add a list to promote inference graph's initializers to global and
   *   mutable variables. Global variables are visible in all graphs of the
   *   stored models.
   * - Add message TrainingInfoProto to store initialization
   *   method and training algorithm. The execution of TrainingInfoProto
   *   can modify the values of mutable variables.
   * - Implicitly add inference graph into each TrainingInfoProto's algorithm.
   */
  IR_VERSION_2020_5_8 = 7,
  /**
   * IR_VERSION_2021_7_30 - IR VERSION 8 published on July 30, 2021
   * Introduce TypeProto.SparseTensor
   * Introduce TypeProto.Optional
   * Added a list of FunctionProtos local to the model
   * Deprecated since_version and operator status from FunctionProto
   */
  IR_VERSION_2021_7_30 = 8,
  /**
   * IR_VERSION_2023_5_5 - IR VERSION 9 published on May 5, 2023
   * Added AttributeProto to FunctionProto so that default attribute values can be set.
   * Added FLOAT8E4M3FN, FLOAT8E4M3FNUZ, FLOAT8E5M2, FLOAT8E5M2FNUZ.
   */
  IR_VERSION_2023_5_5 = 9,
  /**
   * IR_VERSION_2024_3_25 - IR VERSION 10 published on March 25, 2024
   * Added UINT4, INT4.
   */
  IR_VERSION_2024_3_25 = 10,
  /**
   * IR_VERSION - IR VERSION 11 published on TBD
   * Added FLOAT4E2M1.
   */
  IR_VERSION = 11,
  UNRECOGNIZED = -1,
}

export function versionFromJSON(object: any): Version {
  switch (object) {
    case 0:
    case "_START_VERSION":
      return Version._START_VERSION;
    case 1:
    case "IR_VERSION_2017_10_10":
      return Version.IR_VERSION_2017_10_10;
    case 2:
    case "IR_VERSION_2017_10_30":
      return Version.IR_VERSION_2017_10_30;
    case 3:
    case "IR_VERSION_2017_11_3":
      return Version.IR_VERSION_2017_11_3;
    case 4:
    case "IR_VERSION_2019_1_22":
      return Version.IR_VERSION_2019_1_22;
    case 5:
    case "IR_VERSION_2019_3_18":
      return Version.IR_VERSION_2019_3_18;
    case 6:
    case "IR_VERSION_2019_9_19":
      return Version.IR_VERSION_2019_9_19;
    case 7:
    case "IR_VERSION_2020_5_8":
      return Version.IR_VERSION_2020_5_8;
    case 8:
    case "IR_VERSION_2021_7_30":
      return Version.IR_VERSION_2021_7_30;
    case 9:
    case "IR_VERSION_2023_5_5":
      return Version.IR_VERSION_2023_5_5;
    case 10:
    case "IR_VERSION_2024_3_25":
      return Version.IR_VERSION_2024_3_25;
    case 11:
    case "IR_VERSION":
      return Version.IR_VERSION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Version.UNRECOGNIZED;
  }
}

export function versionToJSON(object: Version): string {
  switch (object) {
    case Version._START_VERSION:
      return "_START_VERSION";
    case Version.IR_VERSION_2017_10_10:
      return "IR_VERSION_2017_10_10";
    case Version.IR_VERSION_2017_10_30:
      return "IR_VERSION_2017_10_30";
    case Version.IR_VERSION_2017_11_3:
      return "IR_VERSION_2017_11_3";
    case Version.IR_VERSION_2019_1_22:
      return "IR_VERSION_2019_1_22";
    case Version.IR_VERSION_2019_3_18:
      return "IR_VERSION_2019_3_18";
    case Version.IR_VERSION_2019_9_19:
      return "IR_VERSION_2019_9_19";
    case Version.IR_VERSION_2020_5_8:
      return "IR_VERSION_2020_5_8";
    case Version.IR_VERSION_2021_7_30:
      return "IR_VERSION_2021_7_30";
    case Version.IR_VERSION_2023_5_5:
      return "IR_VERSION_2023_5_5";
    case Version.IR_VERSION_2024_3_25:
      return "IR_VERSION_2024_3_25";
    case Version.IR_VERSION:
      return "IR_VERSION";
    case Version.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Operator/function status. */
export enum OperatorStatus {
  EXPERIMENTAL = 0,
  STABLE = 1,
  UNRECOGNIZED = -1,
}

export function operatorStatusFromJSON(object: any): OperatorStatus {
  switch (object) {
    case 0:
    case "EXPERIMENTAL":
      return OperatorStatus.EXPERIMENTAL;
    case 1:
    case "STABLE":
      return OperatorStatus.STABLE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return OperatorStatus.UNRECOGNIZED;
  }
}

export function operatorStatusToJSON(object: OperatorStatus): string {
  switch (object) {
    case OperatorStatus.EXPERIMENTAL:
      return "EXPERIMENTAL";
    case OperatorStatus.STABLE:
      return "STABLE";
    case OperatorStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Attributes
 *
 * A named attribute containing either singular float, integer, string, graph,
 * and tensor values, or repeated float, integer, string, graph, and tensor values.
 * An AttributeProto MUST contain the name field, and *only one* of the
 * following content fields, effectively enforcing a C/C++ union equivalent.
 */
export interface AttributeProto {
  /** The name field MUST be present for this version of the IR. */
  name: string;
  /**
   * if ref_attr_name is not empty, ref_attr_name is the attribute name in parent function.
   * In this case, this AttributeProto does not contain data, and it's a reference of attribute
   * in parent scope.
   * NOTE: This should ONLY be used in function (sub-graph). It's invalid to be used in main graph.
   */
  refAttrName: string;
  /** A human-readable documentation for this attribute. Markdown is allowed. */
  docString: string;
  /**
   * The type field MUST be present for this version of the IR.
   * For 0.0.1 versions of the IR, this field was not defined, and
   * implementations needed to use has_field heuristics to determine
   * which value field was in use.  For IR_VERSION 0.0.2 or later, this
   * field MUST be set and match the f|i|s|t|... field in use.  This
   * change was made to accommodate proto3 implementations.
   */
  type: AttributeProto_AttributeType;
  /** Exactly ONE of the following fields must be present for this version of the IR */
  f: number;
  /** int */
  i: number;
  /** UTF-8 string */
  s: Uint8Array;
  /** tensor value */
  t:
    | TensorProto
    | undefined;
  /** graph */
  g:
    | GraphProto
    | undefined;
  /** sparse tensor value */
  sparseTensor:
    | SparseTensorProto
    | undefined;
  /**
   * Do not use field below, it's deprecated.
   * optional ValueProto v = 12;         // value - subsumes everything but graph
   */
  tp:
    | TypeProto
    | undefined;
  /** list of floats */
  floats: number[];
  /** list of ints */
  ints: number[];
  /** list of UTF-8 strings */
  strings: Uint8Array[];
  /** list of tensors */
  tensors: TensorProto[];
  /** list of graph */
  graphs: GraphProto[];
  /** list of sparse tensors */
  sparseTensors: SparseTensorProto[];
  /** list of type protos */
  typeProtos: TypeProto[];
}

/**
 * Note: this enum is structurally identical to the OpSchema::AttrType
 * enum defined in schema.h.  If you rev one, you likely need to rev the other.
 */
export enum AttributeProto_AttributeType {
  UNDEFINED = 0,
  FLOAT = 1,
  INT = 2,
  STRING = 3,
  TENSOR = 4,
  GRAPH = 5,
  SPARSE_TENSOR = 11,
  TYPE_PROTO = 13,
  FLOATS = 6,
  INTS = 7,
  STRINGS = 8,
  TENSORS = 9,
  GRAPHS = 10,
  SPARSE_TENSORS = 12,
  TYPE_PROTOS = 14,
  UNRECOGNIZED = -1,
}

export function attributeProto_AttributeTypeFromJSON(object: any): AttributeProto_AttributeType {
  switch (object) {
    case 0:
    case "UNDEFINED":
      return AttributeProto_AttributeType.UNDEFINED;
    case 1:
    case "FLOAT":
      return AttributeProto_AttributeType.FLOAT;
    case 2:
    case "INT":
      return AttributeProto_AttributeType.INT;
    case 3:
    case "STRING":
      return AttributeProto_AttributeType.STRING;
    case 4:
    case "TENSOR":
      return AttributeProto_AttributeType.TENSOR;
    case 5:
    case "GRAPH":
      return AttributeProto_AttributeType.GRAPH;
    case 11:
    case "SPARSE_TENSOR":
      return AttributeProto_AttributeType.SPARSE_TENSOR;
    case 13:
    case "TYPE_PROTO":
      return AttributeProto_AttributeType.TYPE_PROTO;
    case 6:
    case "FLOATS":
      return AttributeProto_AttributeType.FLOATS;
    case 7:
    case "INTS":
      return AttributeProto_AttributeType.INTS;
    case 8:
    case "STRINGS":
      return AttributeProto_AttributeType.STRINGS;
    case 9:
    case "TENSORS":
      return AttributeProto_AttributeType.TENSORS;
    case 10:
    case "GRAPHS":
      return AttributeProto_AttributeType.GRAPHS;
    case 12:
    case "SPARSE_TENSORS":
      return AttributeProto_AttributeType.SPARSE_TENSORS;
    case 14:
    case "TYPE_PROTOS":
      return AttributeProto_AttributeType.TYPE_PROTOS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AttributeProto_AttributeType.UNRECOGNIZED;
  }
}

export function attributeProto_AttributeTypeToJSON(object: AttributeProto_AttributeType): string {
  switch (object) {
    case AttributeProto_AttributeType.UNDEFINED:
      return "UNDEFINED";
    case AttributeProto_AttributeType.FLOAT:
      return "FLOAT";
    case AttributeProto_AttributeType.INT:
      return "INT";
    case AttributeProto_AttributeType.STRING:
      return "STRING";
    case AttributeProto_AttributeType.TENSOR:
      return "TENSOR";
    case AttributeProto_AttributeType.GRAPH:
      return "GRAPH";
    case AttributeProto_AttributeType.SPARSE_TENSOR:
      return "SPARSE_TENSOR";
    case AttributeProto_AttributeType.TYPE_PROTO:
      return "TYPE_PROTO";
    case AttributeProto_AttributeType.FLOATS:
      return "FLOATS";
    case AttributeProto_AttributeType.INTS:
      return "INTS";
    case AttributeProto_AttributeType.STRINGS:
      return "STRINGS";
    case AttributeProto_AttributeType.TENSORS:
      return "TENSORS";
    case AttributeProto_AttributeType.GRAPHS:
      return "GRAPHS";
    case AttributeProto_AttributeType.SPARSE_TENSORS:
      return "SPARSE_TENSORS";
    case AttributeProto_AttributeType.TYPE_PROTOS:
      return "TYPE_PROTOS";
    case AttributeProto_AttributeType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Defines information on value, including the name, the type, and
 * the shape of the value.
 */
export interface ValueInfoProto {
  /** This field MUST be present in this version of the IR. */
  name: string;
  /**
   * This field MUST be present in this version of the IR for
   * inputs and outputs of the top-level graph.
   */
  type:
    | TypeProto
    | undefined;
  /** A human-readable documentation for this value. Markdown is allowed. */
  docString: string;
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
}

/**
 * Nodes
 *
 * Computation graphs are made up of a DAG of nodes, which represent what is
 * commonly called a "layer" or "pipeline stage" in machine learning frameworks.
 *
 * For example, it can be a node of type "Conv" that takes in an image, a filter
 * tensor and a bias tensor, and produces the convolved output.
 */
export interface NodeProto {
  /** namespace Value */
  input: string[];
  /** namespace Value */
  output: string[];
  /**
   * An optional identifier for this node in a graph.
   * This field MAY be absent in this version of the IR.
   */
  name: string;
  /** The symbolic identifier of the Operator to execute. */
  opType: string;
  /** The domain of the OperatorSet that specifies the operator named by op_type. */
  domain: string;
  /** Overload identifier, used only to map this to a model-local function. */
  overload: string;
  /** Additional named attributes. */
  attribute: AttributeProto[];
  /** A human-readable documentation for this node. Markdown is allowed. */
  docString: string;
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
}

/**
 * Training information
 * TrainingInfoProto stores information for training a model.
 * In particular, this defines two functionalities: an initialization-step
 * and a training-algorithm-step. Initialization resets the model
 * back to its original state as if no training has been performed.
 * Training algorithm improves the model based on input data.
 *
 * The semantics of the initialization-step is that the initializers
 * in ModelProto.graph and in TrainingInfoProto.algorithm are first
 * initialized as specified by the initializers in the graph, and then
 * updated by the "initialization_binding" in every instance in
 * ModelProto.training_info.
 *
 * The field "algorithm" defines a computation graph which represents a
 * training algorithm's step. After the execution of a
 * TrainingInfoProto.algorithm, the initializers specified by "update_binding"
 * may be immediately updated. If the targeted training algorithm contains
 * consecutive update steps (such as block coordinate descent methods),
 * the user needs to create a TrainingInfoProto for each step.
 */
export interface TrainingInfoProto {
  /**
   * This field describes a graph to compute the initial tensors
   * upon starting the training process. Initialization graph has no input
   * and can have multiple outputs. Usually, trainable tensors in neural
   * networks are randomly initialized. To achieve that, for each tensor,
   * the user can put a random number operator such as RandomNormal or
   * RandomUniform in TrainingInfoProto.initialization.node and assign its
   * random output to the specific tensor using "initialization_binding".
   * This graph can also set the initializers in "algorithm" in the same
   * TrainingInfoProto; a use case is resetting the number of training
   * iteration to zero.
   *
   * By default, this field is an empty graph and its evaluation does not
   * produce any output. Thus, no initializer would be changed by default.
   */
  initialization:
    | GraphProto
    | undefined;
  /**
   * This field represents a training algorithm step. Given required inputs,
   * it computes outputs to update initializers in its own or inference graph's
   * initializer lists. In general, this field contains loss node, gradient node,
   * optimizer node, increment of iteration count.
   *
   * An execution of the training algorithm step is performed by executing the
   * graph obtained by combining the inference graph (namely "ModelProto.graph")
   * and the "algorithm" graph. That is, the actual
   * input/initializer/output/node/value_info/sparse_initializer list of
   * the training graph is the concatenation of
   * "ModelProto.graph.input/initializer/output/node/value_info/sparse_initializer"
   * and "algorithm.input/initializer/output/node/value_info/sparse_initializer"
   * in that order. This combined graph must satisfy the normal ONNX conditions.
   * Now, let's provide a visualization of graph combination for clarity.
   * Let the inference graph (i.e., "ModelProto.graph") be
   *    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d
   * and the "algorithm" graph be
   *    tensor_d -> Add -> tensor_e
   * The combination process results
   *    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d -> Add -> tensor_e
   *
   * Notice that an input of a node in the "algorithm" graph may reference the
   * output of a node in the inference graph (but not the other way round). Also, inference
   * node cannot reference inputs of "algorithm". With these restrictions, inference graph
   * can always be run independently without training information.
   *
   * By default, this field is an empty graph and its evaluation does not
   * produce any output. Evaluating the default training step never
   * update any initializers.
   */
  algorithm:
    | GraphProto
    | undefined;
  /**
   * This field specifies the bindings from the outputs of "initialization" to
   * some initializers in "ModelProto.graph.initializer" and
   * the "algorithm.initializer" in the same TrainingInfoProto.
   * See "update_binding" below for details.
   *
   * By default, this field is empty and no initializer would be changed
   * by the execution of "initialization".
   */
  initializationBinding: StringStringEntryProto[];
  /**
   * Gradient-based training is usually an iterative procedure. In one gradient
   * descent iteration, we apply
   *
   * x = x - r * g
   *
   * where "x" is the optimized tensor, "r" stands for learning rate, and "g" is
   * gradient of "x" with respect to a chosen loss. To avoid adding assignments
   * into the training graph, we split the update equation into
   *
   * y = x - r * g
   * x = y
   *
   * The user needs to save "y = x - r * g" into TrainingInfoProto.algorithm. To
   * tell that "y" should be assigned to "x", the field "update_binding" may
   * contain a key-value pair of strings, "x" (key of StringStringEntryProto)
   * and "y" (value of StringStringEntryProto).
   * For a neural network with multiple trainable (mutable) tensors, there can
   * be multiple key-value pairs in "update_binding".
   *
   * The initializers appears as keys in "update_binding" are considered
   * mutable variables. This implies some behaviors
   * as described below.
   *
   *  1. We have only unique keys in all "update_binding"s so that two
   *     variables may not have the same name. This ensures that one
   *     variable is assigned up to once.
   *  2. The keys must appear in names of "ModelProto.graph.initializer" or
   *     "TrainingInfoProto.algorithm.initializer".
   *  3. The values must be output names of "algorithm" or "ModelProto.graph.output".
   *  4. Mutable variables are initialized to the value specified by the
   *     corresponding initializer, and then potentially updated by
   *     "initializer_binding"s and "update_binding"s in "TrainingInfoProto"s.
   *
   * This field usually contains names of trainable tensors
   * (in ModelProto.graph), optimizer states such as momentums in advanced
   * stochastic gradient methods (in TrainingInfoProto.graph),
   * and number of training iterations (in TrainingInfoProto.graph).
   *
   * By default, this field is empty and no initializer would be changed
   * by the execution of "algorithm".
   */
  updateBinding: StringStringEntryProto[];
}

/**
 * Models
 *
 * ModelProto is a top-level file/container format for bundling a ML model and
 * associating its computation graph with metadata.
 *
 * The semantics of the model are described by the associated GraphProto's.
 */
export interface ModelProto {
  /**
   * The version of the IR this model targets. See Version enum above.
   * This field MUST be present.
   */
  irVersion: number;
  /**
   * The OperatorSets this model relies on.
   * All ModelProtos MUST have at least one entry that
   * specifies which version of the ONNX OperatorSet is
   * being imported.
   *
   * All nodes in the ModelProto's graph will bind against the operator
   * with the same-domain/same-op_type operator with the HIGHEST version
   * in the referenced operator sets.
   */
  opsetImport: OperatorSetIdProto[];
  /**
   * The name of the framework or tool used to generate this model.
   * This field SHOULD be present to indicate which implementation/tool/framework
   * emitted the model.
   */
  producerName: string;
  /**
   * The version of the framework or tool used to generate this model.
   * This field SHOULD be present to indicate which implementation/tool/framework
   * emitted the model.
   */
  producerVersion: string;
  /**
   * Domain name of the model.
   * We use reverse domain names as name space indicators. For example:
   * `com.facebook.fair` or `com.microsoft.cognitiveservices`
   *
   * Together with `model_version` and GraphProto.name, this forms the unique identity of
   * the graph.
   */
  domain: string;
  /** The version of the graph encoded. See Version enum below. */
  modelVersion: number;
  /** A human-readable documentation for this model. Markdown is allowed. */
  docString: string;
  /** The parameterized graph that is evaluated to execute the model. */
  graph:
    | GraphProto
    | undefined;
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
  /**
   * Training-specific information. Sequentially executing all stored
   * `TrainingInfoProto.algorithm`s and assigning their outputs following
   * the corresponding `TrainingInfoProto.update_binding`s is one training
   * iteration. Similarly, to initialize the model
   * (as if training hasn't happened), the user should sequentially execute
   * all stored `TrainingInfoProto.initialization`s and assigns their outputs
   * using `TrainingInfoProto.initialization_binding`s.
   *
   * If this field is empty, the training behavior of the model is undefined.
   */
  trainingInfo: TrainingInfoProto[];
  /**
   * A list of function protos local to the model.
   *
   * The (domain, name, overload) tuple must be unique across the function protos in this list.
   * In case of any conflicts the behavior (whether the model local functions are given higher priority,
   * or standard operator sets are given higher priotity or this is treated as error) is defined by
   * the runtimes.
   *
   * The operator sets imported by FunctionProto should be compatible with the ones
   * imported by ModelProto and other model local FunctionProtos.
   * Example, if same operator set say 'A' is imported by a FunctionProto and ModelProto
   * or by 2 FunctionProtos then versions for the operator set may be different but,
   * the operator schema returned for op_type, domain, version combination
   * for both the versions should be same for every node in the function body.
   *
   * One FunctionProto can reference other FunctionProto in the model, however, recursive reference
   * is not allowed.
   */
  functions: FunctionProto[];
}

/**
 * StringStringEntryProto follows the pattern for cross-proto-version maps.
 * See https://developers.google.com/protocol-buffers/docs/proto3#maps
 */
export interface StringStringEntryProto {
  key: string;
  value: string;
}

export interface TensorAnnotation {
  tensorName: string;
  /**
   * <key, value> pairs to annotate tensor specified by <tensor_name> above.
   * The keys used in the mapping below must be pre-defined in ONNX spec.
   * For example, for 8-bit linear quantization case, 'SCALE_TENSOR', 'ZERO_POINT_TENSOR' will be pre-defined as
   * quantization parameter keys.
   */
  quantParameterTensorNames: StringStringEntryProto[];
}

/**
 * Graphs
 *
 * A graph defines the computational logic of a model and is comprised of a parameterized
 * list of nodes that form a directed acyclic graph based on their inputs and outputs.
 * This is the equivalent of the "network" or "graph" in many deep learning
 * frameworks.
 */
export interface GraphProto {
  /** The nodes in the graph, sorted topologically. */
  node: NodeProto[];
  /** The name of the graph. */
  name: string;
  /**
   * A list of named tensor values, used to specify constant inputs of the graph.
   * Each initializer (both TensorProto as well SparseTensorProto) MUST have a name.
   * The name MUST be unique across both initializer and sparse_initializer,
   * but the name MAY also appear in the input list.
   */
  initializer: TensorProto[];
  /** Initializers (see above) stored in sparse format. */
  sparseInitializer: SparseTensorProto[];
  /** A human-readable documentation for this graph. Markdown is allowed. */
  docString: string;
  /** The inputs and outputs of the graph. */
  input: ValueInfoProto[];
  output: ValueInfoProto[];
  /**
   * Information for the values in the graph. The ValueInfoProto.name's
   * must be distinct. It is optional for a value to appear in value_info list.
   */
  valueInfo: ValueInfoProto[];
  /**
   * This field carries information to indicate the mapping among a tensor and its
   * quantization parameter tensors. For example:
   * For tensor 'a', it may have {'SCALE_TENSOR', 'a_scale'} and {'ZERO_POINT_TENSOR', 'a_zero_point'} annotated,
   * which means, tensor 'a_scale' and tensor 'a_zero_point' are scale and zero point of tensor 'a' in the model.
   */
  quantizationAnnotation: TensorAnnotation[];
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
}

/**
 * Tensors
 *
 * A serialized tensor value.
 */
export interface TensorProto {
  /** The shape of the tensor. */
  dims: number[];
  /**
   * The data type of the tensor.
   * This field MUST have a valid TensorProto.DataType value
   */
  dataType: number;
  segment:
    | TensorProto_Segment
    | undefined;
  /**
   * For float and complex64 values
   * Complex64 tensors are encoded as a single array of floats,
   * with the real components appearing in odd numbered positions,
   * and the corresponding imaginary component appearing in the
   * subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]
   * is encoded as [1.0, 2.0 ,3.0 ,4.0]
   * When this field is present, the data_type field MUST be FLOAT or COMPLEX64.
   */
  floatData: number[];
  /**
   * For int32, uint8, int8, uint16, int16, uint4, int4, bool, float8 and float16 values
   * float16 and float8 values must be bit-wise converted to an uint16_t prior
   * to writing to the buffer.
   * uint4 and int4 values must be packed to 4bitx2 prior to writing to the buffer, the first element is stored in
   * the 4 LSB and the second element is stored in the 4 MSB.
   * When this field is present, the data_type field MUST be
   * INT32, INT16, INT8, INT4, UINT16, UINT8, UINT4, BOOL, FLOAT16, BFLOAT16, FLOAT8E4M3FN, FLOAT8E4M3FNUZ, FLOAT8E5M2, FLOAT8E5M2FNUZ
   */
  int32Data: number[];
  /**
   * For strings.
   * Each element of string_data is a UTF-8 encoded Unicode
   * string. No trailing null, no leading BOM. The protobuf "string"
   * scalar type is not used to match ML community conventions.
   * When this field is present, the data_type field MUST be STRING
   */
  stringData: Uint8Array[];
  /**
   * For int64.
   * When this field is present, the data_type field MUST be INT64
   */
  int64Data: number[];
  /** Optionally, a name for the tensor. */
  name: string;
  /** A human-readable documentation for this tensor. Markdown is allowed. */
  docString: string;
  /**
   * Serializations can either use one of the fields above, or use this
   * raw bytes field. The only exception is the string case, where one is
   * required to store the content in the repeated bytes string_data field.
   *
   * When this raw_data field is used to store tensor value, elements MUST
   * be stored in as fixed-width, little-endian order.
   * Floating-point data types MUST be stored in IEEE 754 format.
   * Complex64 elements must be written as two consecutive FLOAT values, real component first.
   * Complex128 elements must be written as two consecutive DOUBLE values, real component first.
   * Boolean type MUST be written one byte per tensor element (00000001 for true, 00000000 for false).
   * uint4 and int4 values must be packed to 4bitx2, the first element is stored in the 4 LSB and the second element is stored in the 4 MSB.
   *
   * Note: the advantage of specific field rather than the raw_data field is
   * that in some cases (e.g. int data), protobuf does a better packing via
   * variable length storage, and may lead to smaller binary footprint.
   * When this field is present, the data_type field MUST NOT be STRING or UNDEFINED
   */
  rawData: Uint8Array;
  /**
   * Data can be stored inside the protobuf file using type-specific fields or raw_data.
   * Alternatively, raw bytes data can be stored in an external file, using the external_data field.
   * external_data stores key-value pairs describing data location. Recognized keys are:
   * - "location" (required) - POSIX filesystem path relative to the directory where the ONNX
   *                           protobuf model was stored
   * - "offset" (optional) - position of byte at which stored data begins. Integer stored as string.
   *                         Offset values SHOULD be multiples 4096 (page size) to enable mmap support.
   * - "length" (optional) - number of bytes containing data. Integer stored as string.
   * - "checksum" (optional) - SHA1 digest of file specified in under 'location' key.
   */
  externalData: StringStringEntryProto[];
  /** If value not set, data is stored in raw_data (if set) otherwise in type-specified field. */
  dataLocation: TensorProto_DataLocation;
  /**
   * For double
   * Complex128 tensors are encoded as a single array of doubles,
   * with the real components appearing in odd numbered positions,
   * and the corresponding imaginary component appearing in the
   * subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]
   * is encoded as [1.0, 2.0 ,3.0 ,4.0]
   * When this field is present, the data_type field MUST be DOUBLE or COMPLEX128
   */
  doubleData: number[];
  /**
   * For uint64 and uint32 values
   * When this field is present, the data_type field MUST be
   * UINT32 or UINT64
   */
  uint64Data: number[];
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
}

export enum TensorProto_DataType {
  UNDEFINED = 0,
  /** FLOAT - Basic types. */
  FLOAT = 1,
  /** UINT8 - uint8_t */
  UINT8 = 2,
  /** INT8 - int8_t */
  INT8 = 3,
  /** UINT16 - uint16_t */
  UINT16 = 4,
  /** INT16 - int16_t */
  INT16 = 5,
  /** INT32 - int32_t */
  INT32 = 6,
  /** INT64 - int64_t */
  INT64 = 7,
  /** STRING - string */
  STRING = 8,
  /** BOOL - bool */
  BOOL = 9,
  /**
   * FLOAT16 - IEEE754 half-precision floating-point format (16 bits wide).
   * This format has 1 sign bit, 5 exponent bits, and 10 mantissa bits.
   */
  FLOAT16 = 10,
  DOUBLE = 11,
  UINT32 = 12,
  UINT64 = 13,
  /** COMPLEX64 - complex with float32 real and imaginary components */
  COMPLEX64 = 14,
  /** COMPLEX128 - complex with float64 real and imaginary components */
  COMPLEX128 = 15,
  /**
   * BFLOAT16 - Non-IEEE floating-point format based on IEEE754 single-precision
   * floating-point number truncated to 16 bits.
   * This format has 1 sign bit, 8 exponent bits, and 7 mantissa bits.
   */
  BFLOAT16 = 16,
  /**
   * FLOAT8E4M3FN - Non-IEEE floating-point format based on papers
   * FP8 Formats for Deep Learning, https://arxiv.org/abs/2209.05433,
   * 8-bit Numerical Formats For Deep Neural Networks, https://arxiv.org/pdf/2206.02915.pdf.
   * Operators supported FP8 are Cast, CastLike, QuantizeLinear, DequantizeLinear.
   * The computation usually happens inside a block quantize / dequantize
   * fused by the runtime.
   */
  FLOAT8E4M3FN = 17,
  /** FLOAT8E4M3FNUZ - float 8, mostly used for coefficients, supports nan, not inf, no negative zero */
  FLOAT8E4M3FNUZ = 18,
  /** FLOAT8E5M2 - follows IEEE 754, supports nan, inf, mostly used for gradients */
  FLOAT8E5M2 = 19,
  /** FLOAT8E5M2FNUZ - follows IEEE 754, supports nan, not inf, mostly used for gradients, no negative zero */
  FLOAT8E5M2FNUZ = 20,
  /** UINT4 - 4-bit integer data types */
  UINT4 = 21,
  /** INT4 - Signed integer in range [-8, 7], using two's-complement representation */
  INT4 = 22,
  /** FLOAT4E2M1 - 4-bit floating point data types */
  FLOAT4E2M1 = 23,
  UNRECOGNIZED = -1,
}

export function tensorProto_DataTypeFromJSON(object: any): TensorProto_DataType {
  switch (object) {
    case 0:
    case "UNDEFINED":
      return TensorProto_DataType.UNDEFINED;
    case 1:
    case "FLOAT":
      return TensorProto_DataType.FLOAT;
    case 2:
    case "UINT8":
      return TensorProto_DataType.UINT8;
    case 3:
    case "INT8":
      return TensorProto_DataType.INT8;
    case 4:
    case "UINT16":
      return TensorProto_DataType.UINT16;
    case 5:
    case "INT16":
      return TensorProto_DataType.INT16;
    case 6:
    case "INT32":
      return TensorProto_DataType.INT32;
    case 7:
    case "INT64":
      return TensorProto_DataType.INT64;
    case 8:
    case "STRING":
      return TensorProto_DataType.STRING;
    case 9:
    case "BOOL":
      return TensorProto_DataType.BOOL;
    case 10:
    case "FLOAT16":
      return TensorProto_DataType.FLOAT16;
    case 11:
    case "DOUBLE":
      return TensorProto_DataType.DOUBLE;
    case 12:
    case "UINT32":
      return TensorProto_DataType.UINT32;
    case 13:
    case "UINT64":
      return TensorProto_DataType.UINT64;
    case 14:
    case "COMPLEX64":
      return TensorProto_DataType.COMPLEX64;
    case 15:
    case "COMPLEX128":
      return TensorProto_DataType.COMPLEX128;
    case 16:
    case "BFLOAT16":
      return TensorProto_DataType.BFLOAT16;
    case 17:
    case "FLOAT8E4M3FN":
      return TensorProto_DataType.FLOAT8E4M3FN;
    case 18:
    case "FLOAT8E4M3FNUZ":
      return TensorProto_DataType.FLOAT8E4M3FNUZ;
    case 19:
    case "FLOAT8E5M2":
      return TensorProto_DataType.FLOAT8E5M2;
    case 20:
    case "FLOAT8E5M2FNUZ":
      return TensorProto_DataType.FLOAT8E5M2FNUZ;
    case 21:
    case "UINT4":
      return TensorProto_DataType.UINT4;
    case 22:
    case "INT4":
      return TensorProto_DataType.INT4;
    case 23:
    case "FLOAT4E2M1":
      return TensorProto_DataType.FLOAT4E2M1;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TensorProto_DataType.UNRECOGNIZED;
  }
}

export function tensorProto_DataTypeToJSON(object: TensorProto_DataType): string {
  switch (object) {
    case TensorProto_DataType.UNDEFINED:
      return "UNDEFINED";
    case TensorProto_DataType.FLOAT:
      return "FLOAT";
    case TensorProto_DataType.UINT8:
      return "UINT8";
    case TensorProto_DataType.INT8:
      return "INT8";
    case TensorProto_DataType.UINT16:
      return "UINT16";
    case TensorProto_DataType.INT16:
      return "INT16";
    case TensorProto_DataType.INT32:
      return "INT32";
    case TensorProto_DataType.INT64:
      return "INT64";
    case TensorProto_DataType.STRING:
      return "STRING";
    case TensorProto_DataType.BOOL:
      return "BOOL";
    case TensorProto_DataType.FLOAT16:
      return "FLOAT16";
    case TensorProto_DataType.DOUBLE:
      return "DOUBLE";
    case TensorProto_DataType.UINT32:
      return "UINT32";
    case TensorProto_DataType.UINT64:
      return "UINT64";
    case TensorProto_DataType.COMPLEX64:
      return "COMPLEX64";
    case TensorProto_DataType.COMPLEX128:
      return "COMPLEX128";
    case TensorProto_DataType.BFLOAT16:
      return "BFLOAT16";
    case TensorProto_DataType.FLOAT8E4M3FN:
      return "FLOAT8E4M3FN";
    case TensorProto_DataType.FLOAT8E4M3FNUZ:
      return "FLOAT8E4M3FNUZ";
    case TensorProto_DataType.FLOAT8E5M2:
      return "FLOAT8E5M2";
    case TensorProto_DataType.FLOAT8E5M2FNUZ:
      return "FLOAT8E5M2FNUZ";
    case TensorProto_DataType.UINT4:
      return "UINT4";
    case TensorProto_DataType.INT4:
      return "INT4";
    case TensorProto_DataType.FLOAT4E2M1:
      return "FLOAT4E2M1";
    case TensorProto_DataType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Location of the data for this tensor. MUST be one of:
 * - DEFAULT - data stored inside the protobuf message. Data is stored in raw_data (if set) otherwise in type-specified field.
 * - EXTERNAL - data stored in an external location as described by external_data field.
 */
export enum TensorProto_DataLocation {
  DEFAULT = 0,
  EXTERNAL = 1,
  UNRECOGNIZED = -1,
}

export function tensorProto_DataLocationFromJSON(object: any): TensorProto_DataLocation {
  switch (object) {
    case 0:
    case "DEFAULT":
      return TensorProto_DataLocation.DEFAULT;
    case 1:
    case "EXTERNAL":
      return TensorProto_DataLocation.EXTERNAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TensorProto_DataLocation.UNRECOGNIZED;
  }
}

export function tensorProto_DataLocationToJSON(object: TensorProto_DataLocation): string {
  switch (object) {
    case TensorProto_DataLocation.DEFAULT:
      return "DEFAULT";
    case TensorProto_DataLocation.EXTERNAL:
      return "EXTERNAL";
    case TensorProto_DataLocation.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * For very large tensors, we may want to store them in chunks, in which
 * case the following fields will specify the segment that is stored in
 * the current TensorProto.
 */
export interface TensorProto_Segment {
  begin: number;
  end: number;
}

/** A serialized sparse-tensor value */
export interface SparseTensorProto {
  /**
   * The sequence of non-default values are encoded as a tensor of shape [NNZ].
   * The default-value is zero for numeric tensors, and empty-string for string tensors.
   * values must have a non-empty name present which serves as a name for SparseTensorProto
   * when used in sparse_initializer list.
   */
  values:
    | TensorProto
    | undefined;
  /**
   * The indices of the non-default values, which may be stored in one of two formats.
   * (a) Indices can be a tensor of shape [NNZ, rank] with the [i,j]-th value
   * corresponding to the j-th index of the i-th value (in the values tensor).
   * (b) Indices can be a tensor of shape [NNZ], in which case the i-th value
   * must be the linearized-index of the i-th value (in the values tensor).
   * The linearized-index can be converted into an index tuple (k_1,...,k_rank)
   * using the shape provided below.
   * The indices must appear in ascending order without duplication.
   * In the first format, the ordering is lexicographic-ordering:
   * e.g., index-value [1,4] must appear before [2,1]
   */
  indices:
    | TensorProto
    | undefined;
  /** The shape of the underlying dense-tensor: [dim_1, dim_2, ... dim_rank] */
  dims: number[];
}

/**
 * Defines a tensor shape. A dimension can be either an integer value
 * or a symbolic variable. A symbolic variable represents an unknown
 * dimension.
 */
export interface TensorShapeProto {
  dim: TensorShapeProto_Dimension[];
}

export interface TensorShapeProto_Dimension {
  dimValue?:
    | number
    | undefined;
  /** namespace Shape */
  dimParam?:
    | string
    | undefined;
  /**
   * Standard denotation can optionally be used to denote tensor
   * dimensions with standard semantic descriptions to ensure
   * that operations are applied to the correct axis of a tensor.
   * Refer to https://github.com/onnx/onnx/blob/main/docs/DimensionDenotation.md#denotation-definition
   * for pre-defined dimension denotations.
   */
  denotation: string;
}

/**
 * Types
 *
 * The standard ONNX data types.
 */
export interface TypeProto {
  /** The type of a tensor. */
  tensorType?:
    | TypeProto_Tensor
    | undefined;
  /** The type of a sequence. */
  sequenceType?:
    | TypeProto_Sequence
    | undefined;
  /** The type of a map. */
  mapType?:
    | TypeProto_Map
    | undefined;
  /** The type of an optional. */
  optionalType?:
    | TypeProto_Optional
    | undefined;
  /** Type of the sparse tensor */
  sparseTensorType?:
    | TypeProto_SparseTensor
    | undefined;
  /**
   * An optional denotation can be used to denote the whole
   * type with a standard semantic description as to what is
   * stored inside. Refer to https://github.com/onnx/onnx/blob/main/docs/TypeDenotation.md#type-denotation-definition
   * for pre-defined type denotations.
   */
  denotation: string;
}

export interface TypeProto_Tensor {
  /**
   * This field MUST NOT have the value of UNDEFINED
   * This field MUST have a valid TensorProto.DataType value
   * This field MUST be present for this version of the IR.
   */
  elemType: number;
  shape: TensorShapeProto | undefined;
}

/** repeated T */
export interface TypeProto_Sequence {
  /**
   * The type and optional shape of each element of the sequence.
   * This field MUST be present for this version of the IR.
   */
  elemType: TypeProto | undefined;
}

/** map<K,V> */
export interface TypeProto_Map {
  /**
   * This field MUST have a valid TensorProto.DataType value
   * This field MUST be present for this version of the IR.
   * This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING
   */
  keyType: number;
  /** This field MUST be present for this version of the IR. */
  valueType: TypeProto | undefined;
}

/** wrapper for Tensor, Sequence, or Map */
export interface TypeProto_Optional {
  /**
   * The type and optional shape of the element wrapped.
   * This field MUST be present for this version of the IR.
   * Possible values correspond to OptionalProto.DataType enum
   */
  elemType: TypeProto | undefined;
}

export interface TypeProto_SparseTensor {
  /**
   * This field MUST NOT have the value of UNDEFINED
   * This field MUST have a valid TensorProto.DataType value
   * This field MUST be present for this version of the IR.
   */
  elemType: number;
  shape: TensorShapeProto | undefined;
}

/**
 * Operator Sets
 *
 * OperatorSets are uniquely identified by a (domain, opset_version) pair.
 */
export interface OperatorSetIdProto {
  /**
   * The domain of the operator set being identified.
   * The empty string ("") or absence of this field implies the operator
   * set that is defined as part of the ONNX specification.
   * This field MUST be present in this version of the IR when referring to any other operator set.
   */
  domain: string;
  /**
   * The version of the operator set being identified.
   * This field MUST be present in this version of the IR.
   */
  version: number;
}

export interface FunctionProto {
  /**
   * The name of the function, similar to op_type in NodeProto.
   * This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.
   */
  name: string;
  /** The inputs and outputs of the function. */
  input: string[];
  output: string[];
  /**
   * The attribute parameters of the function.
   * It is for function parameters without default values.
   */
  attribute: string[];
  /**
   * The attribute protos of the function.
   * It is for function attributes with default values.
   * A function attribute shall be represented either as
   * a string attribute or an AttributeProto, not both.
   */
  attributeProto: AttributeProto[];
  /** The nodes in the function. */
  node: NodeProto[];
  /** A human-readable documentation for this function. Markdown is allowed. */
  docString: string;
  opsetImport: OperatorSetIdProto[];
  /**
   * The domain which this function belongs to.
   * This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.
   */
  domain: string;
  /**
   * The overload identifier of the function.
   * This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.
   */
  overload: string;
  /**
   * Information for the values in the function. The ValueInfoProto.name's
   * must be distinct and refer to names in the function (including inputs,
   * outputs, and intermediate values). It is optional for a value to appear
   * in value_info list.
   */
  valueInfo: ValueInfoProto[];
  /** Named metadata values; keys should be distinct. */
  metadataProps: StringStringEntryProto[];
}

function createBaseAttributeProto(): AttributeProto {
  return {
    name: "",
    refAttrName: "",
    docString: "",
    type: 0,
    f: 0,
    i: 0,
    s: new Uint8Array(0),
    t: undefined,
    g: undefined,
    sparseTensor: undefined,
    tp: undefined,
    floats: [],
    ints: [],
    strings: [],
    tensors: [],
    graphs: [],
    sparseTensors: [],
    typeProtos: [],
  };
}

export const AttributeProto: MessageFns<AttributeProto> = {
  encode(message: AttributeProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.refAttrName !== "") {
      writer.uint32(170).string(message.refAttrName);
    }
    if (message.docString !== "") {
      writer.uint32(106).string(message.docString);
    }
    if (message.type !== 0) {
      writer.uint32(160).int32(message.type);
    }
    if (message.f !== 0) {
      writer.uint32(21).float(message.f);
    }
    if (message.i !== 0) {
      writer.uint32(24).int64(message.i);
    }
    if (message.s.length !== 0) {
      writer.uint32(34).bytes(message.s);
    }
    if (message.t !== undefined) {
      TensorProto.encode(message.t, writer.uint32(42).fork()).join();
    }
    if (message.g !== undefined) {
      GraphProto.encode(message.g, writer.uint32(50).fork()).join();
    }
    if (message.sparseTensor !== undefined) {
      SparseTensorProto.encode(message.sparseTensor, writer.uint32(178).fork()).join();
    }
    if (message.tp !== undefined) {
      TypeProto.encode(message.tp, writer.uint32(114).fork()).join();
    }
    writer.uint32(58).fork();
    for (const v of message.floats) {
      writer.float(v);
    }
    writer.join();
    writer.uint32(66).fork();
    for (const v of message.ints) {
      writer.int64(v);
    }
    writer.join();
    for (const v of message.strings) {
      writer.uint32(74).bytes(v!);
    }
    for (const v of message.tensors) {
      TensorProto.encode(v!, writer.uint32(82).fork()).join();
    }
    for (const v of message.graphs) {
      GraphProto.encode(v!, writer.uint32(90).fork()).join();
    }
    for (const v of message.sparseTensors) {
      SparseTensorProto.encode(v!, writer.uint32(186).fork()).join();
    }
    for (const v of message.typeProtos) {
      TypeProto.encode(v!, writer.uint32(122).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AttributeProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAttributeProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 21: {
          if (tag !== 170) {
            break;
          }

          message.refAttrName = reader.string();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 20: {
          if (tag !== 160) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 21) {
            break;
          }

          message.f = reader.float();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.i = longToNumber(reader.int64());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.s = reader.bytes();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.t = TensorProto.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.g = GraphProto.decode(reader, reader.uint32());
          continue;
        }
        case 22: {
          if (tag !== 178) {
            break;
          }

          message.sparseTensor = SparseTensorProto.decode(reader, reader.uint32());
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.tp = TypeProto.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag === 61) {
            message.floats.push(reader.float());

            continue;
          }

          if (tag === 58) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.floats.push(reader.float());
            }

            continue;
          }

          break;
        }
        case 8: {
          if (tag === 64) {
            message.ints.push(longToNumber(reader.int64()));

            continue;
          }

          if (tag === 66) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.ints.push(longToNumber(reader.int64()));
            }

            continue;
          }

          break;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.strings.push(reader.bytes());
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.tensors.push(TensorProto.decode(reader, reader.uint32()));
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.graphs.push(GraphProto.decode(reader, reader.uint32()));
          continue;
        }
        case 23: {
          if (tag !== 186) {
            break;
          }

          message.sparseTensors.push(SparseTensorProto.decode(reader, reader.uint32()));
          continue;
        }
        case 15: {
          if (tag !== 122) {
            break;
          }

          message.typeProtos.push(TypeProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AttributeProto {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      refAttrName: isSet(object.refAttrName) ? globalThis.String(object.refAttrName) : "",
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      type: isSet(object.type) ? attributeProto_AttributeTypeFromJSON(object.type) : 0,
      f: isSet(object.f) ? globalThis.Number(object.f) : 0,
      i: isSet(object.i) ? globalThis.Number(object.i) : 0,
      s: isSet(object.s) ? bytesFromBase64(object.s) : new Uint8Array(0),
      t: isSet(object.t) ? TensorProto.fromJSON(object.t) : undefined,
      g: isSet(object.g) ? GraphProto.fromJSON(object.g) : undefined,
      sparseTensor: isSet(object.sparseTensor) ? SparseTensorProto.fromJSON(object.sparseTensor) : undefined,
      tp: isSet(object.tp) ? TypeProto.fromJSON(object.tp) : undefined,
      floats: globalThis.Array.isArray(object?.floats) ? object.floats.map((e: any) => globalThis.Number(e)) : [],
      ints: globalThis.Array.isArray(object?.ints) ? object.ints.map((e: any) => globalThis.Number(e)) : [],
      strings: globalThis.Array.isArray(object?.strings) ? object.strings.map((e: any) => bytesFromBase64(e)) : [],
      tensors: globalThis.Array.isArray(object?.tensors) ? object.tensors.map((e: any) => TensorProto.fromJSON(e)) : [],
      graphs: globalThis.Array.isArray(object?.graphs) ? object.graphs.map((e: any) => GraphProto.fromJSON(e)) : [],
      sparseTensors: globalThis.Array.isArray(object?.sparseTensors)
        ? object.sparseTensors.map((e: any) => SparseTensorProto.fromJSON(e))
        : [],
      typeProtos: globalThis.Array.isArray(object?.typeProtos)
        ? object.typeProtos.map((e: any) => TypeProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AttributeProto): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.refAttrName !== "") {
      obj.refAttrName = message.refAttrName;
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.type !== 0) {
      obj.type = attributeProto_AttributeTypeToJSON(message.type);
    }
    if (message.f !== 0) {
      obj.f = message.f;
    }
    if (message.i !== 0) {
      obj.i = Math.round(message.i);
    }
    if (message.s.length !== 0) {
      obj.s = base64FromBytes(message.s);
    }
    if (message.t !== undefined) {
      obj.t = TensorProto.toJSON(message.t);
    }
    if (message.g !== undefined) {
      obj.g = GraphProto.toJSON(message.g);
    }
    if (message.sparseTensor !== undefined) {
      obj.sparseTensor = SparseTensorProto.toJSON(message.sparseTensor);
    }
    if (message.tp !== undefined) {
      obj.tp = TypeProto.toJSON(message.tp);
    }
    if (message.floats?.length) {
      obj.floats = message.floats;
    }
    if (message.ints?.length) {
      obj.ints = message.ints.map((e) => Math.round(e));
    }
    if (message.strings?.length) {
      obj.strings = message.strings.map((e) => base64FromBytes(e));
    }
    if (message.tensors?.length) {
      obj.tensors = message.tensors.map((e) => TensorProto.toJSON(e));
    }
    if (message.graphs?.length) {
      obj.graphs = message.graphs.map((e) => GraphProto.toJSON(e));
    }
    if (message.sparseTensors?.length) {
      obj.sparseTensors = message.sparseTensors.map((e) => SparseTensorProto.toJSON(e));
    }
    if (message.typeProtos?.length) {
      obj.typeProtos = message.typeProtos.map((e) => TypeProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AttributeProto>, I>>(base?: I): AttributeProto {
    return AttributeProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AttributeProto>, I>>(object: I): AttributeProto {
    const message = createBaseAttributeProto();
    message.name = object.name ?? "";
    message.refAttrName = object.refAttrName ?? "";
    message.docString = object.docString ?? "";
    message.type = object.type ?? 0;
    message.f = object.f ?? 0;
    message.i = object.i ?? 0;
    message.s = object.s ?? new Uint8Array(0);
    message.t = (object.t !== undefined && object.t !== null) ? TensorProto.fromPartial(object.t) : undefined;
    message.g = (object.g !== undefined && object.g !== null) ? GraphProto.fromPartial(object.g) : undefined;
    message.sparseTensor = (object.sparseTensor !== undefined && object.sparseTensor !== null)
      ? SparseTensorProto.fromPartial(object.sparseTensor)
      : undefined;
    message.tp = (object.tp !== undefined && object.tp !== null) ? TypeProto.fromPartial(object.tp) : undefined;
    message.floats = object.floats?.map((e) => e) || [];
    message.ints = object.ints?.map((e) => e) || [];
    message.strings = object.strings?.map((e) => e) || [];
    message.tensors = object.tensors?.map((e) => TensorProto.fromPartial(e)) || [];
    message.graphs = object.graphs?.map((e) => GraphProto.fromPartial(e)) || [];
    message.sparseTensors = object.sparseTensors?.map((e) => SparseTensorProto.fromPartial(e)) || [];
    message.typeProtos = object.typeProtos?.map((e) => TypeProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseValueInfoProto(): ValueInfoProto {
  return { name: "", type: undefined, docString: "", metadataProps: [] };
}

export const ValueInfoProto: MessageFns<ValueInfoProto> = {
  encode(message: ValueInfoProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.type !== undefined) {
      TypeProto.encode(message.type, writer.uint32(18).fork()).join();
    }
    if (message.docString !== "") {
      writer.uint32(26).string(message.docString);
    }
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ValueInfoProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseValueInfoProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.type = TypeProto.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ValueInfoProto {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? TypeProto.fromJSON(object.type) : undefined,
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ValueInfoProto): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== undefined) {
      obj.type = TypeProto.toJSON(message.type);
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ValueInfoProto>, I>>(base?: I): ValueInfoProto {
    return ValueInfoProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ValueInfoProto>, I>>(object: I): ValueInfoProto {
    const message = createBaseValueInfoProto();
    message.name = object.name ?? "";
    message.type = (object.type !== undefined && object.type !== null) ? TypeProto.fromPartial(object.type) : undefined;
    message.docString = object.docString ?? "";
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseNodeProto(): NodeProto {
  return {
    input: [],
    output: [],
    name: "",
    opType: "",
    domain: "",
    overload: "",
    attribute: [],
    docString: "",
    metadataProps: [],
  };
}

export const NodeProto: MessageFns<NodeProto> = {
  encode(message: NodeProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.input) {
      writer.uint32(10).string(v!);
    }
    for (const v of message.output) {
      writer.uint32(18).string(v!);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.opType !== "") {
      writer.uint32(34).string(message.opType);
    }
    if (message.domain !== "") {
      writer.uint32(58).string(message.domain);
    }
    if (message.overload !== "") {
      writer.uint32(66).string(message.overload);
    }
    for (const v of message.attribute) {
      AttributeProto.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.docString !== "") {
      writer.uint32(50).string(message.docString);
    }
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.input.push(reader.string());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.output.push(reader.string());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.opType = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.domain = reader.string();
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.overload = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.attribute.push(AttributeProto.decode(reader, reader.uint32()));
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeProto {
    return {
      input: globalThis.Array.isArray(object?.input) ? object.input.map((e: any) => globalThis.String(e)) : [],
      output: globalThis.Array.isArray(object?.output) ? object.output.map((e: any) => globalThis.String(e)) : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      opType: isSet(object.opType) ? globalThis.String(object.opType) : "",
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      overload: isSet(object.overload) ? globalThis.String(object.overload) : "",
      attribute: globalThis.Array.isArray(object?.attribute)
        ? object.attribute.map((e: any) => AttributeProto.fromJSON(e))
        : [],
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: NodeProto): unknown {
    const obj: any = {};
    if (message.input?.length) {
      obj.input = message.input;
    }
    if (message.output?.length) {
      obj.output = message.output;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.opType !== "") {
      obj.opType = message.opType;
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.overload !== "") {
      obj.overload = message.overload;
    }
    if (message.attribute?.length) {
      obj.attribute = message.attribute.map((e) => AttributeProto.toJSON(e));
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<NodeProto>, I>>(base?: I): NodeProto {
    return NodeProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<NodeProto>, I>>(object: I): NodeProto {
    const message = createBaseNodeProto();
    message.input = object.input?.map((e) => e) || [];
    message.output = object.output?.map((e) => e) || [];
    message.name = object.name ?? "";
    message.opType = object.opType ?? "";
    message.domain = object.domain ?? "";
    message.overload = object.overload ?? "";
    message.attribute = object.attribute?.map((e) => AttributeProto.fromPartial(e)) || [];
    message.docString = object.docString ?? "";
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTrainingInfoProto(): TrainingInfoProto {
  return { initialization: undefined, algorithm: undefined, initializationBinding: [], updateBinding: [] };
}

export const TrainingInfoProto: MessageFns<TrainingInfoProto> = {
  encode(message: TrainingInfoProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.initialization !== undefined) {
      GraphProto.encode(message.initialization, writer.uint32(10).fork()).join();
    }
    if (message.algorithm !== undefined) {
      GraphProto.encode(message.algorithm, writer.uint32(18).fork()).join();
    }
    for (const v of message.initializationBinding) {
      StringStringEntryProto.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.updateBinding) {
      StringStringEntryProto.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TrainingInfoProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrainingInfoProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.initialization = GraphProto.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.algorithm = GraphProto.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.initializationBinding.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.updateBinding.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TrainingInfoProto {
    return {
      initialization: isSet(object.initialization) ? GraphProto.fromJSON(object.initialization) : undefined,
      algorithm: isSet(object.algorithm) ? GraphProto.fromJSON(object.algorithm) : undefined,
      initializationBinding: globalThis.Array.isArray(object?.initializationBinding)
        ? object.initializationBinding.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
      updateBinding: globalThis.Array.isArray(object?.updateBinding)
        ? object.updateBinding.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TrainingInfoProto): unknown {
    const obj: any = {};
    if (message.initialization !== undefined) {
      obj.initialization = GraphProto.toJSON(message.initialization);
    }
    if (message.algorithm !== undefined) {
      obj.algorithm = GraphProto.toJSON(message.algorithm);
    }
    if (message.initializationBinding?.length) {
      obj.initializationBinding = message.initializationBinding.map((e) => StringStringEntryProto.toJSON(e));
    }
    if (message.updateBinding?.length) {
      obj.updateBinding = message.updateBinding.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TrainingInfoProto>, I>>(base?: I): TrainingInfoProto {
    return TrainingInfoProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TrainingInfoProto>, I>>(object: I): TrainingInfoProto {
    const message = createBaseTrainingInfoProto();
    message.initialization = (object.initialization !== undefined && object.initialization !== null)
      ? GraphProto.fromPartial(object.initialization)
      : undefined;
    message.algorithm = (object.algorithm !== undefined && object.algorithm !== null)
      ? GraphProto.fromPartial(object.algorithm)
      : undefined;
    message.initializationBinding = object.initializationBinding?.map((e) => StringStringEntryProto.fromPartial(e)) ||
      [];
    message.updateBinding = object.updateBinding?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseModelProto(): ModelProto {
  return {
    irVersion: 0,
    opsetImport: [],
    producerName: "",
    producerVersion: "",
    domain: "",
    modelVersion: 0,
    docString: "",
    graph: undefined,
    metadataProps: [],
    trainingInfo: [],
    functions: [],
  };
}

export const ModelProto: MessageFns<ModelProto> = {
  encode(message: ModelProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.irVersion !== 0) {
      writer.uint32(8).int64(message.irVersion);
    }
    for (const v of message.opsetImport) {
      OperatorSetIdProto.encode(v!, writer.uint32(66).fork()).join();
    }
    if (message.producerName !== "") {
      writer.uint32(18).string(message.producerName);
    }
    if (message.producerVersion !== "") {
      writer.uint32(26).string(message.producerVersion);
    }
    if (message.domain !== "") {
      writer.uint32(34).string(message.domain);
    }
    if (message.modelVersion !== 0) {
      writer.uint32(40).int64(message.modelVersion);
    }
    if (message.docString !== "") {
      writer.uint32(50).string(message.docString);
    }
    if (message.graph !== undefined) {
      GraphProto.encode(message.graph, writer.uint32(58).fork()).join();
    }
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(114).fork()).join();
    }
    for (const v of message.trainingInfo) {
      TrainingInfoProto.encode(v!, writer.uint32(162).fork()).join();
    }
    for (const v of message.functions) {
      FunctionProto.encode(v!, writer.uint32(202).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModelProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModelProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.irVersion = longToNumber(reader.int64());
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.opsetImport.push(OperatorSetIdProto.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.producerName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.producerVersion = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.domain = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.modelVersion = longToNumber(reader.int64());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.graph = GraphProto.decode(reader, reader.uint32());
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
        case 20: {
          if (tag !== 162) {
            break;
          }

          message.trainingInfo.push(TrainingInfoProto.decode(reader, reader.uint32()));
          continue;
        }
        case 25: {
          if (tag !== 202) {
            break;
          }

          message.functions.push(FunctionProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModelProto {
    return {
      irVersion: isSet(object.irVersion) ? globalThis.Number(object.irVersion) : 0,
      opsetImport: globalThis.Array.isArray(object?.opsetImport)
        ? object.opsetImport.map((e: any) => OperatorSetIdProto.fromJSON(e))
        : [],
      producerName: isSet(object.producerName) ? globalThis.String(object.producerName) : "",
      producerVersion: isSet(object.producerVersion) ? globalThis.String(object.producerVersion) : "",
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      modelVersion: isSet(object.modelVersion) ? globalThis.Number(object.modelVersion) : 0,
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      graph: isSet(object.graph) ? GraphProto.fromJSON(object.graph) : undefined,
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
      trainingInfo: globalThis.Array.isArray(object?.trainingInfo)
        ? object.trainingInfo.map((e: any) => TrainingInfoProto.fromJSON(e))
        : [],
      functions: globalThis.Array.isArray(object?.functions)
        ? object.functions.map((e: any) => FunctionProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ModelProto): unknown {
    const obj: any = {};
    if (message.irVersion !== 0) {
      obj.irVersion = Math.round(message.irVersion);
    }
    if (message.opsetImport?.length) {
      obj.opsetImport = message.opsetImport.map((e) => OperatorSetIdProto.toJSON(e));
    }
    if (message.producerName !== "") {
      obj.producerName = message.producerName;
    }
    if (message.producerVersion !== "") {
      obj.producerVersion = message.producerVersion;
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.modelVersion !== 0) {
      obj.modelVersion = Math.round(message.modelVersion);
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.graph !== undefined) {
      obj.graph = GraphProto.toJSON(message.graph);
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    if (message.trainingInfo?.length) {
      obj.trainingInfo = message.trainingInfo.map((e) => TrainingInfoProto.toJSON(e));
    }
    if (message.functions?.length) {
      obj.functions = message.functions.map((e) => FunctionProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ModelProto>, I>>(base?: I): ModelProto {
    return ModelProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ModelProto>, I>>(object: I): ModelProto {
    const message = createBaseModelProto();
    message.irVersion = object.irVersion ?? 0;
    message.opsetImport = object.opsetImport?.map((e) => OperatorSetIdProto.fromPartial(e)) || [];
    message.producerName = object.producerName ?? "";
    message.producerVersion = object.producerVersion ?? "";
    message.domain = object.domain ?? "";
    message.modelVersion = object.modelVersion ?? 0;
    message.docString = object.docString ?? "";
    message.graph = (object.graph !== undefined && object.graph !== null)
      ? GraphProto.fromPartial(object.graph)
      : undefined;
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    message.trainingInfo = object.trainingInfo?.map((e) => TrainingInfoProto.fromPartial(e)) || [];
    message.functions = object.functions?.map((e) => FunctionProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStringStringEntryProto(): StringStringEntryProto {
  return { key: "", value: "" };
}

export const StringStringEntryProto: MessageFns<StringStringEntryProto> = {
  encode(message: StringStringEntryProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StringStringEntryProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStringStringEntryProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StringStringEntryProto {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: StringStringEntryProto): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StringStringEntryProto>, I>>(base?: I): StringStringEntryProto {
    return StringStringEntryProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StringStringEntryProto>, I>>(object: I): StringStringEntryProto {
    const message = createBaseStringStringEntryProto();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseTensorAnnotation(): TensorAnnotation {
  return { tensorName: "", quantParameterTensorNames: [] };
}

export const TensorAnnotation: MessageFns<TensorAnnotation> = {
  encode(message: TensorAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tensorName !== "") {
      writer.uint32(10).string(message.tensorName);
    }
    for (const v of message.quantParameterTensorNames) {
      StringStringEntryProto.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TensorAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTensorAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.tensorName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.quantParameterTensorNames.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TensorAnnotation {
    return {
      tensorName: isSet(object.tensorName) ? globalThis.String(object.tensorName) : "",
      quantParameterTensorNames: globalThis.Array.isArray(object?.quantParameterTensorNames)
        ? object.quantParameterTensorNames.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TensorAnnotation): unknown {
    const obj: any = {};
    if (message.tensorName !== "") {
      obj.tensorName = message.tensorName;
    }
    if (message.quantParameterTensorNames?.length) {
      obj.quantParameterTensorNames = message.quantParameterTensorNames.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TensorAnnotation>, I>>(base?: I): TensorAnnotation {
    return TensorAnnotation.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TensorAnnotation>, I>>(object: I): TensorAnnotation {
    const message = createBaseTensorAnnotation();
    message.tensorName = object.tensorName ?? "";
    message.quantParameterTensorNames =
      object.quantParameterTensorNames?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGraphProto(): GraphProto {
  return {
    node: [],
    name: "",
    initializer: [],
    sparseInitializer: [],
    docString: "",
    input: [],
    output: [],
    valueInfo: [],
    quantizationAnnotation: [],
    metadataProps: [],
  };
}

export const GraphProto: MessageFns<GraphProto> = {
  encode(message: GraphProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.node) {
      NodeProto.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    for (const v of message.initializer) {
      TensorProto.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.sparseInitializer) {
      SparseTensorProto.encode(v!, writer.uint32(122).fork()).join();
    }
    if (message.docString !== "") {
      writer.uint32(82).string(message.docString);
    }
    for (const v of message.input) {
      ValueInfoProto.encode(v!, writer.uint32(90).fork()).join();
    }
    for (const v of message.output) {
      ValueInfoProto.encode(v!, writer.uint32(98).fork()).join();
    }
    for (const v of message.valueInfo) {
      ValueInfoProto.encode(v!, writer.uint32(106).fork()).join();
    }
    for (const v of message.quantizationAnnotation) {
      TensorAnnotation.encode(v!, writer.uint32(114).fork()).join();
    }
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(130).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GraphProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGraphProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.node.push(NodeProto.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.initializer.push(TensorProto.decode(reader, reader.uint32()));
          continue;
        }
        case 15: {
          if (tag !== 122) {
            break;
          }

          message.sparseInitializer.push(SparseTensorProto.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.input.push(ValueInfoProto.decode(reader, reader.uint32()));
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.output.push(ValueInfoProto.decode(reader, reader.uint32()));
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.valueInfo.push(ValueInfoProto.decode(reader, reader.uint32()));
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.quantizationAnnotation.push(TensorAnnotation.decode(reader, reader.uint32()));
          continue;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GraphProto {
    return {
      node: globalThis.Array.isArray(object?.node) ? object.node.map((e: any) => NodeProto.fromJSON(e)) : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      initializer: globalThis.Array.isArray(object?.initializer)
        ? object.initializer.map((e: any) => TensorProto.fromJSON(e))
        : [],
      sparseInitializer: globalThis.Array.isArray(object?.sparseInitializer)
        ? object.sparseInitializer.map((e: any) => SparseTensorProto.fromJSON(e))
        : [],
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      input: globalThis.Array.isArray(object?.input) ? object.input.map((e: any) => ValueInfoProto.fromJSON(e)) : [],
      output: globalThis.Array.isArray(object?.output) ? object.output.map((e: any) => ValueInfoProto.fromJSON(e)) : [],
      valueInfo: globalThis.Array.isArray(object?.valueInfo)
        ? object.valueInfo.map((e: any) => ValueInfoProto.fromJSON(e))
        : [],
      quantizationAnnotation: globalThis.Array.isArray(object?.quantizationAnnotation)
        ? object.quantizationAnnotation.map((e: any) => TensorAnnotation.fromJSON(e))
        : [],
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GraphProto): unknown {
    const obj: any = {};
    if (message.node?.length) {
      obj.node = message.node.map((e) => NodeProto.toJSON(e));
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.initializer?.length) {
      obj.initializer = message.initializer.map((e) => TensorProto.toJSON(e));
    }
    if (message.sparseInitializer?.length) {
      obj.sparseInitializer = message.sparseInitializer.map((e) => SparseTensorProto.toJSON(e));
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.input?.length) {
      obj.input = message.input.map((e) => ValueInfoProto.toJSON(e));
    }
    if (message.output?.length) {
      obj.output = message.output.map((e) => ValueInfoProto.toJSON(e));
    }
    if (message.valueInfo?.length) {
      obj.valueInfo = message.valueInfo.map((e) => ValueInfoProto.toJSON(e));
    }
    if (message.quantizationAnnotation?.length) {
      obj.quantizationAnnotation = message.quantizationAnnotation.map((e) => TensorAnnotation.toJSON(e));
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<GraphProto>, I>>(base?: I): GraphProto {
    return GraphProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<GraphProto>, I>>(object: I): GraphProto {
    const message = createBaseGraphProto();
    message.node = object.node?.map((e) => NodeProto.fromPartial(e)) || [];
    message.name = object.name ?? "";
    message.initializer = object.initializer?.map((e) => TensorProto.fromPartial(e)) || [];
    message.sparseInitializer = object.sparseInitializer?.map((e) => SparseTensorProto.fromPartial(e)) || [];
    message.docString = object.docString ?? "";
    message.input = object.input?.map((e) => ValueInfoProto.fromPartial(e)) || [];
    message.output = object.output?.map((e) => ValueInfoProto.fromPartial(e)) || [];
    message.valueInfo = object.valueInfo?.map((e) => ValueInfoProto.fromPartial(e)) || [];
    message.quantizationAnnotation = object.quantizationAnnotation?.map((e) => TensorAnnotation.fromPartial(e)) || [];
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTensorProto(): TensorProto {
  return {
    dims: [],
    dataType: 0,
    segment: undefined,
    floatData: [],
    int32Data: [],
    stringData: [],
    int64Data: [],
    name: "",
    docString: "",
    rawData: new Uint8Array(0),
    externalData: [],
    dataLocation: 0,
    doubleData: [],
    uint64Data: [],
    metadataProps: [],
  };
}

export const TensorProto: MessageFns<TensorProto> = {
  encode(message: TensorProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.dims) {
      writer.int64(v);
    }
    writer.join();
    if (message.dataType !== 0) {
      writer.uint32(16).int32(message.dataType);
    }
    if (message.segment !== undefined) {
      TensorProto_Segment.encode(message.segment, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.floatData) {
      writer.float(v);
    }
    writer.join();
    writer.uint32(42).fork();
    for (const v of message.int32Data) {
      writer.int32(v);
    }
    writer.join();
    for (const v of message.stringData) {
      writer.uint32(50).bytes(v!);
    }
    writer.uint32(58).fork();
    for (const v of message.int64Data) {
      writer.int64(v);
    }
    writer.join();
    if (message.name !== "") {
      writer.uint32(66).string(message.name);
    }
    if (message.docString !== "") {
      writer.uint32(98).string(message.docString);
    }
    if (message.rawData.length !== 0) {
      writer.uint32(74).bytes(message.rawData);
    }
    for (const v of message.externalData) {
      StringStringEntryProto.encode(v!, writer.uint32(106).fork()).join();
    }
    if (message.dataLocation !== 0) {
      writer.uint32(112).int32(message.dataLocation);
    }
    writer.uint32(82).fork();
    for (const v of message.doubleData) {
      writer.double(v);
    }
    writer.join();
    writer.uint32(90).fork();
    for (const v of message.uint64Data) {
      writer.uint64(v);
    }
    writer.join();
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(130).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TensorProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTensorProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag === 8) {
            message.dims.push(longToNumber(reader.int64()));

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.dims.push(longToNumber(reader.int64()));
            }

            continue;
          }

          break;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.dataType = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.segment = TensorProto_Segment.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag === 37) {
            message.floatData.push(reader.float());

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.floatData.push(reader.float());
            }

            continue;
          }

          break;
        }
        case 5: {
          if (tag === 40) {
            message.int32Data.push(reader.int32());

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.int32Data.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.stringData.push(reader.bytes());
          continue;
        }
        case 7: {
          if (tag === 56) {
            message.int64Data.push(longToNumber(reader.int64()));

            continue;
          }

          if (tag === 58) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.int64Data.push(longToNumber(reader.int64()));
            }

            continue;
          }

          break;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.rawData = reader.bytes();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.externalData.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
        case 14: {
          if (tag !== 112) {
            break;
          }

          message.dataLocation = reader.int32() as any;
          continue;
        }
        case 10: {
          if (tag === 81) {
            message.doubleData.push(reader.double());

            continue;
          }

          if (tag === 82) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.doubleData.push(reader.double());
            }

            continue;
          }

          break;
        }
        case 11: {
          if (tag === 88) {
            message.uint64Data.push(longToNumber(reader.uint64()));

            continue;
          }

          if (tag === 90) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.uint64Data.push(longToNumber(reader.uint64()));
            }

            continue;
          }

          break;
        }
        case 16: {
          if (tag !== 130) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TensorProto {
    return {
      dims: globalThis.Array.isArray(object?.dims) ? object.dims.map((e: any) => globalThis.Number(e)) : [],
      dataType: isSet(object.dataType) ? globalThis.Number(object.dataType) : 0,
      segment: isSet(object.segment) ? TensorProto_Segment.fromJSON(object.segment) : undefined,
      floatData: globalThis.Array.isArray(object?.floatData)
        ? object.floatData.map((e: any) => globalThis.Number(e))
        : [],
      int32Data: globalThis.Array.isArray(object?.int32Data)
        ? object.int32Data.map((e: any) => globalThis.Number(e))
        : [],
      stringData: globalThis.Array.isArray(object?.stringData)
        ? object.stringData.map((e: any) => bytesFromBase64(e))
        : [],
      int64Data: globalThis.Array.isArray(object?.int64Data)
        ? object.int64Data.map((e: any) => globalThis.Number(e))
        : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      rawData: isSet(object.rawData) ? bytesFromBase64(object.rawData) : new Uint8Array(0),
      externalData: globalThis.Array.isArray(object?.externalData)
        ? object.externalData.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
      dataLocation: isSet(object.dataLocation) ? tensorProto_DataLocationFromJSON(object.dataLocation) : 0,
      doubleData: globalThis.Array.isArray(object?.doubleData)
        ? object.doubleData.map((e: any) => globalThis.Number(e))
        : [],
      uint64Data: globalThis.Array.isArray(object?.uint64Data)
        ? object.uint64Data.map((e: any) => globalThis.Number(e))
        : [],
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TensorProto): unknown {
    const obj: any = {};
    if (message.dims?.length) {
      obj.dims = message.dims.map((e) => Math.round(e));
    }
    if (message.dataType !== 0) {
      obj.dataType = Math.round(message.dataType);
    }
    if (message.segment !== undefined) {
      obj.segment = TensorProto_Segment.toJSON(message.segment);
    }
    if (message.floatData?.length) {
      obj.floatData = message.floatData;
    }
    if (message.int32Data?.length) {
      obj.int32Data = message.int32Data.map((e) => Math.round(e));
    }
    if (message.stringData?.length) {
      obj.stringData = message.stringData.map((e) => base64FromBytes(e));
    }
    if (message.int64Data?.length) {
      obj.int64Data = message.int64Data.map((e) => Math.round(e));
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.rawData.length !== 0) {
      obj.rawData = base64FromBytes(message.rawData);
    }
    if (message.externalData?.length) {
      obj.externalData = message.externalData.map((e) => StringStringEntryProto.toJSON(e));
    }
    if (message.dataLocation !== 0) {
      obj.dataLocation = tensorProto_DataLocationToJSON(message.dataLocation);
    }
    if (message.doubleData?.length) {
      obj.doubleData = message.doubleData;
    }
    if (message.uint64Data?.length) {
      obj.uint64Data = message.uint64Data.map((e) => Math.round(e));
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TensorProto>, I>>(base?: I): TensorProto {
    return TensorProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TensorProto>, I>>(object: I): TensorProto {
    const message = createBaseTensorProto();
    message.dims = object.dims?.map((e) => e) || [];
    message.dataType = object.dataType ?? 0;
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? TensorProto_Segment.fromPartial(object.segment)
      : undefined;
    message.floatData = object.floatData?.map((e) => e) || [];
    message.int32Data = object.int32Data?.map((e) => e) || [];
    message.stringData = object.stringData?.map((e) => e) || [];
    message.int64Data = object.int64Data?.map((e) => e) || [];
    message.name = object.name ?? "";
    message.docString = object.docString ?? "";
    message.rawData = object.rawData ?? new Uint8Array(0);
    message.externalData = object.externalData?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    message.dataLocation = object.dataLocation ?? 0;
    message.doubleData = object.doubleData?.map((e) => e) || [];
    message.uint64Data = object.uint64Data?.map((e) => e) || [];
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTensorProto_Segment(): TensorProto_Segment {
  return { begin: 0, end: 0 };
}

export const TensorProto_Segment: MessageFns<TensorProto_Segment> = {
  encode(message: TensorProto_Segment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.begin !== 0) {
      writer.uint32(8).int64(message.begin);
    }
    if (message.end !== 0) {
      writer.uint32(16).int64(message.end);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TensorProto_Segment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTensorProto_Segment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.begin = longToNumber(reader.int64());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.end = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TensorProto_Segment {
    return {
      begin: isSet(object.begin) ? globalThis.Number(object.begin) : 0,
      end: isSet(object.end) ? globalThis.Number(object.end) : 0,
    };
  },

  toJSON(message: TensorProto_Segment): unknown {
    const obj: any = {};
    if (message.begin !== 0) {
      obj.begin = Math.round(message.begin);
    }
    if (message.end !== 0) {
      obj.end = Math.round(message.end);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TensorProto_Segment>, I>>(base?: I): TensorProto_Segment {
    return TensorProto_Segment.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TensorProto_Segment>, I>>(object: I): TensorProto_Segment {
    const message = createBaseTensorProto_Segment();
    message.begin = object.begin ?? 0;
    message.end = object.end ?? 0;
    return message;
  },
};

function createBaseSparseTensorProto(): SparseTensorProto {
  return { values: undefined, indices: undefined, dims: [] };
}

export const SparseTensorProto: MessageFns<SparseTensorProto> = {
  encode(message: SparseTensorProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.values !== undefined) {
      TensorProto.encode(message.values, writer.uint32(10).fork()).join();
    }
    if (message.indices !== undefined) {
      TensorProto.encode(message.indices, writer.uint32(18).fork()).join();
    }
    writer.uint32(26).fork();
    for (const v of message.dims) {
      writer.int64(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparseTensorProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparseTensorProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.values = TensorProto.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.indices = TensorProto.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag === 24) {
            message.dims.push(longToNumber(reader.int64()));

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.dims.push(longToNumber(reader.int64()));
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparseTensorProto {
    return {
      values: isSet(object.values) ? TensorProto.fromJSON(object.values) : undefined,
      indices: isSet(object.indices) ? TensorProto.fromJSON(object.indices) : undefined,
      dims: globalThis.Array.isArray(object?.dims) ? object.dims.map((e: any) => globalThis.Number(e)) : [],
    };
  },

  toJSON(message: SparseTensorProto): unknown {
    const obj: any = {};
    if (message.values !== undefined) {
      obj.values = TensorProto.toJSON(message.values);
    }
    if (message.indices !== undefined) {
      obj.indices = TensorProto.toJSON(message.indices);
    }
    if (message.dims?.length) {
      obj.dims = message.dims.map((e) => Math.round(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SparseTensorProto>, I>>(base?: I): SparseTensorProto {
    return SparseTensorProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SparseTensorProto>, I>>(object: I): SparseTensorProto {
    const message = createBaseSparseTensorProto();
    message.values = (object.values !== undefined && object.values !== null)
      ? TensorProto.fromPartial(object.values)
      : undefined;
    message.indices = (object.indices !== undefined && object.indices !== null)
      ? TensorProto.fromPartial(object.indices)
      : undefined;
    message.dims = object.dims?.map((e) => e) || [];
    return message;
  },
};

function createBaseTensorShapeProto(): TensorShapeProto {
  return { dim: [] };
}

export const TensorShapeProto: MessageFns<TensorShapeProto> = {
  encode(message: TensorShapeProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.dim) {
      TensorShapeProto_Dimension.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TensorShapeProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTensorShapeProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.dim.push(TensorShapeProto_Dimension.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TensorShapeProto {
    return {
      dim: globalThis.Array.isArray(object?.dim)
        ? object.dim.map((e: any) => TensorShapeProto_Dimension.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TensorShapeProto): unknown {
    const obj: any = {};
    if (message.dim?.length) {
      obj.dim = message.dim.map((e) => TensorShapeProto_Dimension.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TensorShapeProto>, I>>(base?: I): TensorShapeProto {
    return TensorShapeProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TensorShapeProto>, I>>(object: I): TensorShapeProto {
    const message = createBaseTensorShapeProto();
    message.dim = object.dim?.map((e) => TensorShapeProto_Dimension.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTensorShapeProto_Dimension(): TensorShapeProto_Dimension {
  return { dimValue: undefined, dimParam: undefined, denotation: "" };
}

export const TensorShapeProto_Dimension: MessageFns<TensorShapeProto_Dimension> = {
  encode(message: TensorShapeProto_Dimension, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dimValue !== undefined) {
      writer.uint32(8).int64(message.dimValue);
    }
    if (message.dimParam !== undefined) {
      writer.uint32(18).string(message.dimParam);
    }
    if (message.denotation !== "") {
      writer.uint32(26).string(message.denotation);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TensorShapeProto_Dimension {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTensorShapeProto_Dimension();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.dimValue = longToNumber(reader.int64());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.dimParam = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.denotation = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TensorShapeProto_Dimension {
    return {
      dimValue: isSet(object.dimValue) ? globalThis.Number(object.dimValue) : undefined,
      dimParam: isSet(object.dimParam) ? globalThis.String(object.dimParam) : undefined,
      denotation: isSet(object.denotation) ? globalThis.String(object.denotation) : "",
    };
  },

  toJSON(message: TensorShapeProto_Dimension): unknown {
    const obj: any = {};
    if (message.dimValue !== undefined) {
      obj.dimValue = Math.round(message.dimValue);
    }
    if (message.dimParam !== undefined) {
      obj.dimParam = message.dimParam;
    }
    if (message.denotation !== "") {
      obj.denotation = message.denotation;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TensorShapeProto_Dimension>, I>>(base?: I): TensorShapeProto_Dimension {
    return TensorShapeProto_Dimension.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TensorShapeProto_Dimension>, I>>(object: I): TensorShapeProto_Dimension {
    const message = createBaseTensorShapeProto_Dimension();
    message.dimValue = object.dimValue ?? undefined;
    message.dimParam = object.dimParam ?? undefined;
    message.denotation = object.denotation ?? "";
    return message;
  },
};

function createBaseTypeProto(): TypeProto {
  return {
    tensorType: undefined,
    sequenceType: undefined,
    mapType: undefined,
    optionalType: undefined,
    sparseTensorType: undefined,
    denotation: "",
  };
}

export const TypeProto: MessageFns<TypeProto> = {
  encode(message: TypeProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tensorType !== undefined) {
      TypeProto_Tensor.encode(message.tensorType, writer.uint32(10).fork()).join();
    }
    if (message.sequenceType !== undefined) {
      TypeProto_Sequence.encode(message.sequenceType, writer.uint32(34).fork()).join();
    }
    if (message.mapType !== undefined) {
      TypeProto_Map.encode(message.mapType, writer.uint32(42).fork()).join();
    }
    if (message.optionalType !== undefined) {
      TypeProto_Optional.encode(message.optionalType, writer.uint32(74).fork()).join();
    }
    if (message.sparseTensorType !== undefined) {
      TypeProto_SparseTensor.encode(message.sparseTensorType, writer.uint32(66).fork()).join();
    }
    if (message.denotation !== "") {
      writer.uint32(50).string(message.denotation);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.tensorType = TypeProto_Tensor.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.sequenceType = TypeProto_Sequence.decode(reader, reader.uint32());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.mapType = TypeProto_Map.decode(reader, reader.uint32());
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.optionalType = TypeProto_Optional.decode(reader, reader.uint32());
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.sparseTensorType = TypeProto_SparseTensor.decode(reader, reader.uint32());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.denotation = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto {
    return {
      tensorType: isSet(object.tensorType) ? TypeProto_Tensor.fromJSON(object.tensorType) : undefined,
      sequenceType: isSet(object.sequenceType) ? TypeProto_Sequence.fromJSON(object.sequenceType) : undefined,
      mapType: isSet(object.mapType) ? TypeProto_Map.fromJSON(object.mapType) : undefined,
      optionalType: isSet(object.optionalType) ? TypeProto_Optional.fromJSON(object.optionalType) : undefined,
      sparseTensorType: isSet(object.sparseTensorType)
        ? TypeProto_SparseTensor.fromJSON(object.sparseTensorType)
        : undefined,
      denotation: isSet(object.denotation) ? globalThis.String(object.denotation) : "",
    };
  },

  toJSON(message: TypeProto): unknown {
    const obj: any = {};
    if (message.tensorType !== undefined) {
      obj.tensorType = TypeProto_Tensor.toJSON(message.tensorType);
    }
    if (message.sequenceType !== undefined) {
      obj.sequenceType = TypeProto_Sequence.toJSON(message.sequenceType);
    }
    if (message.mapType !== undefined) {
      obj.mapType = TypeProto_Map.toJSON(message.mapType);
    }
    if (message.optionalType !== undefined) {
      obj.optionalType = TypeProto_Optional.toJSON(message.optionalType);
    }
    if (message.sparseTensorType !== undefined) {
      obj.sparseTensorType = TypeProto_SparseTensor.toJSON(message.sparseTensorType);
    }
    if (message.denotation !== "") {
      obj.denotation = message.denotation;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto>, I>>(base?: I): TypeProto {
    return TypeProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto>, I>>(object: I): TypeProto {
    const message = createBaseTypeProto();
    message.tensorType = (object.tensorType !== undefined && object.tensorType !== null)
      ? TypeProto_Tensor.fromPartial(object.tensorType)
      : undefined;
    message.sequenceType = (object.sequenceType !== undefined && object.sequenceType !== null)
      ? TypeProto_Sequence.fromPartial(object.sequenceType)
      : undefined;
    message.mapType = (object.mapType !== undefined && object.mapType !== null)
      ? TypeProto_Map.fromPartial(object.mapType)
      : undefined;
    message.optionalType = (object.optionalType !== undefined && object.optionalType !== null)
      ? TypeProto_Optional.fromPartial(object.optionalType)
      : undefined;
    message.sparseTensorType = (object.sparseTensorType !== undefined && object.sparseTensorType !== null)
      ? TypeProto_SparseTensor.fromPartial(object.sparseTensorType)
      : undefined;
    message.denotation = object.denotation ?? "";
    return message;
  },
};

function createBaseTypeProto_Tensor(): TypeProto_Tensor {
  return { elemType: 0, shape: undefined };
}

export const TypeProto_Tensor: MessageFns<TypeProto_Tensor> = {
  encode(message: TypeProto_Tensor, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.elemType !== 0) {
      writer.uint32(8).int32(message.elemType);
    }
    if (message.shape !== undefined) {
      TensorShapeProto.encode(message.shape, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto_Tensor {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto_Tensor();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.elemType = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.shape = TensorShapeProto.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto_Tensor {
    return {
      elemType: isSet(object.elemType) ? globalThis.Number(object.elemType) : 0,
      shape: isSet(object.shape) ? TensorShapeProto.fromJSON(object.shape) : undefined,
    };
  },

  toJSON(message: TypeProto_Tensor): unknown {
    const obj: any = {};
    if (message.elemType !== 0) {
      obj.elemType = Math.round(message.elemType);
    }
    if (message.shape !== undefined) {
      obj.shape = TensorShapeProto.toJSON(message.shape);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto_Tensor>, I>>(base?: I): TypeProto_Tensor {
    return TypeProto_Tensor.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto_Tensor>, I>>(object: I): TypeProto_Tensor {
    const message = createBaseTypeProto_Tensor();
    message.elemType = object.elemType ?? 0;
    message.shape = (object.shape !== undefined && object.shape !== null)
      ? TensorShapeProto.fromPartial(object.shape)
      : undefined;
    return message;
  },
};

function createBaseTypeProto_Sequence(): TypeProto_Sequence {
  return { elemType: undefined };
}

export const TypeProto_Sequence: MessageFns<TypeProto_Sequence> = {
  encode(message: TypeProto_Sequence, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.elemType !== undefined) {
      TypeProto.encode(message.elemType, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto_Sequence {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto_Sequence();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.elemType = TypeProto.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto_Sequence {
    return { elemType: isSet(object.elemType) ? TypeProto.fromJSON(object.elemType) : undefined };
  },

  toJSON(message: TypeProto_Sequence): unknown {
    const obj: any = {};
    if (message.elemType !== undefined) {
      obj.elemType = TypeProto.toJSON(message.elemType);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto_Sequence>, I>>(base?: I): TypeProto_Sequence {
    return TypeProto_Sequence.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto_Sequence>, I>>(object: I): TypeProto_Sequence {
    const message = createBaseTypeProto_Sequence();
    message.elemType = (object.elemType !== undefined && object.elemType !== null)
      ? TypeProto.fromPartial(object.elemType)
      : undefined;
    return message;
  },
};

function createBaseTypeProto_Map(): TypeProto_Map {
  return { keyType: 0, valueType: undefined };
}

export const TypeProto_Map: MessageFns<TypeProto_Map> = {
  encode(message: TypeProto_Map, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.keyType !== 0) {
      writer.uint32(8).int32(message.keyType);
    }
    if (message.valueType !== undefined) {
      TypeProto.encode(message.valueType, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto_Map {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto_Map();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.keyType = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.valueType = TypeProto.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto_Map {
    return {
      keyType: isSet(object.keyType) ? globalThis.Number(object.keyType) : 0,
      valueType: isSet(object.valueType) ? TypeProto.fromJSON(object.valueType) : undefined,
    };
  },

  toJSON(message: TypeProto_Map): unknown {
    const obj: any = {};
    if (message.keyType !== 0) {
      obj.keyType = Math.round(message.keyType);
    }
    if (message.valueType !== undefined) {
      obj.valueType = TypeProto.toJSON(message.valueType);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto_Map>, I>>(base?: I): TypeProto_Map {
    return TypeProto_Map.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto_Map>, I>>(object: I): TypeProto_Map {
    const message = createBaseTypeProto_Map();
    message.keyType = object.keyType ?? 0;
    message.valueType = (object.valueType !== undefined && object.valueType !== null)
      ? TypeProto.fromPartial(object.valueType)
      : undefined;
    return message;
  },
};

function createBaseTypeProto_Optional(): TypeProto_Optional {
  return { elemType: undefined };
}

export const TypeProto_Optional: MessageFns<TypeProto_Optional> = {
  encode(message: TypeProto_Optional, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.elemType !== undefined) {
      TypeProto.encode(message.elemType, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto_Optional {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto_Optional();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.elemType = TypeProto.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto_Optional {
    return { elemType: isSet(object.elemType) ? TypeProto.fromJSON(object.elemType) : undefined };
  },

  toJSON(message: TypeProto_Optional): unknown {
    const obj: any = {};
    if (message.elemType !== undefined) {
      obj.elemType = TypeProto.toJSON(message.elemType);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto_Optional>, I>>(base?: I): TypeProto_Optional {
    return TypeProto_Optional.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto_Optional>, I>>(object: I): TypeProto_Optional {
    const message = createBaseTypeProto_Optional();
    message.elemType = (object.elemType !== undefined && object.elemType !== null)
      ? TypeProto.fromPartial(object.elemType)
      : undefined;
    return message;
  },
};

function createBaseTypeProto_SparseTensor(): TypeProto_SparseTensor {
  return { elemType: 0, shape: undefined };
}

export const TypeProto_SparseTensor: MessageFns<TypeProto_SparseTensor> = {
  encode(message: TypeProto_SparseTensor, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.elemType !== 0) {
      writer.uint32(8).int32(message.elemType);
    }
    if (message.shape !== undefined) {
      TensorShapeProto.encode(message.shape, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypeProto_SparseTensor {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypeProto_SparseTensor();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.elemType = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.shape = TensorShapeProto.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypeProto_SparseTensor {
    return {
      elemType: isSet(object.elemType) ? globalThis.Number(object.elemType) : 0,
      shape: isSet(object.shape) ? TensorShapeProto.fromJSON(object.shape) : undefined,
    };
  },

  toJSON(message: TypeProto_SparseTensor): unknown {
    const obj: any = {};
    if (message.elemType !== 0) {
      obj.elemType = Math.round(message.elemType);
    }
    if (message.shape !== undefined) {
      obj.shape = TensorShapeProto.toJSON(message.shape);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TypeProto_SparseTensor>, I>>(base?: I): TypeProto_SparseTensor {
    return TypeProto_SparseTensor.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TypeProto_SparseTensor>, I>>(object: I): TypeProto_SparseTensor {
    const message = createBaseTypeProto_SparseTensor();
    message.elemType = object.elemType ?? 0;
    message.shape = (object.shape !== undefined && object.shape !== null)
      ? TensorShapeProto.fromPartial(object.shape)
      : undefined;
    return message;
  },
};

function createBaseOperatorSetIdProto(): OperatorSetIdProto {
  return { domain: "", version: 0 };
}

export const OperatorSetIdProto: MessageFns<OperatorSetIdProto> = {
  encode(message: OperatorSetIdProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.domain !== "") {
      writer.uint32(10).string(message.domain);
    }
    if (message.version !== 0) {
      writer.uint32(16).int64(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperatorSetIdProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperatorSetIdProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.domain = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.version = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperatorSetIdProto {
    return {
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      version: isSet(object.version) ? globalThis.Number(object.version) : 0,
    };
  },

  toJSON(message: OperatorSetIdProto): unknown {
    const obj: any = {};
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.version !== 0) {
      obj.version = Math.round(message.version);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<OperatorSetIdProto>, I>>(base?: I): OperatorSetIdProto {
    return OperatorSetIdProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<OperatorSetIdProto>, I>>(object: I): OperatorSetIdProto {
    const message = createBaseOperatorSetIdProto();
    message.domain = object.domain ?? "";
    message.version = object.version ?? 0;
    return message;
  },
};

function createBaseFunctionProto(): FunctionProto {
  return {
    name: "",
    input: [],
    output: [],
    attribute: [],
    attributeProto: [],
    node: [],
    docString: "",
    opsetImport: [],
    domain: "",
    overload: "",
    valueInfo: [],
    metadataProps: [],
  };
}

export const FunctionProto: MessageFns<FunctionProto> = {
  encode(message: FunctionProto, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    for (const v of message.input) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.output) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.attribute) {
      writer.uint32(50).string(v!);
    }
    for (const v of message.attributeProto) {
      AttributeProto.encode(v!, writer.uint32(90).fork()).join();
    }
    for (const v of message.node) {
      NodeProto.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.docString !== "") {
      writer.uint32(66).string(message.docString);
    }
    for (const v of message.opsetImport) {
      OperatorSetIdProto.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.domain !== "") {
      writer.uint32(82).string(message.domain);
    }
    if (message.overload !== "") {
      writer.uint32(106).string(message.overload);
    }
    for (const v of message.valueInfo) {
      ValueInfoProto.encode(v!, writer.uint32(98).fork()).join();
    }
    for (const v of message.metadataProps) {
      StringStringEntryProto.encode(v!, writer.uint32(114).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FunctionProto {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFunctionProto();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.input.push(reader.string());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.output.push(reader.string());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.attribute.push(reader.string());
          continue;
        }
        case 11: {
          if (tag !== 90) {
            break;
          }

          message.attributeProto.push(AttributeProto.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.node.push(NodeProto.decode(reader, reader.uint32()));
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.docString = reader.string();
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.opsetImport.push(OperatorSetIdProto.decode(reader, reader.uint32()));
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.domain = reader.string();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.overload = reader.string();
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.valueInfo.push(ValueInfoProto.decode(reader, reader.uint32()));
          continue;
        }
        case 14: {
          if (tag !== 114) {
            break;
          }

          message.metadataProps.push(StringStringEntryProto.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FunctionProto {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      input: globalThis.Array.isArray(object?.input) ? object.input.map((e: any) => globalThis.String(e)) : [],
      output: globalThis.Array.isArray(object?.output) ? object.output.map((e: any) => globalThis.String(e)) : [],
      attribute: globalThis.Array.isArray(object?.attribute)
        ? object.attribute.map((e: any) => globalThis.String(e))
        : [],
      attributeProto: globalThis.Array.isArray(object?.attributeProto)
        ? object.attributeProto.map((e: any) => AttributeProto.fromJSON(e))
        : [],
      node: globalThis.Array.isArray(object?.node) ? object.node.map((e: any) => NodeProto.fromJSON(e)) : [],
      docString: isSet(object.docString) ? globalThis.String(object.docString) : "",
      opsetImport: globalThis.Array.isArray(object?.opsetImport)
        ? object.opsetImport.map((e: any) => OperatorSetIdProto.fromJSON(e))
        : [],
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      overload: isSet(object.overload) ? globalThis.String(object.overload) : "",
      valueInfo: globalThis.Array.isArray(object?.valueInfo)
        ? object.valueInfo.map((e: any) => ValueInfoProto.fromJSON(e))
        : [],
      metadataProps: globalThis.Array.isArray(object?.metadataProps)
        ? object.metadataProps.map((e: any) => StringStringEntryProto.fromJSON(e))
        : [],
    };
  },

  toJSON(message: FunctionProto): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.input?.length) {
      obj.input = message.input;
    }
    if (message.output?.length) {
      obj.output = message.output;
    }
    if (message.attribute?.length) {
      obj.attribute = message.attribute;
    }
    if (message.attributeProto?.length) {
      obj.attributeProto = message.attributeProto.map((e) => AttributeProto.toJSON(e));
    }
    if (message.node?.length) {
      obj.node = message.node.map((e) => NodeProto.toJSON(e));
    }
    if (message.docString !== "") {
      obj.docString = message.docString;
    }
    if (message.opsetImport?.length) {
      obj.opsetImport = message.opsetImport.map((e) => OperatorSetIdProto.toJSON(e));
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.overload !== "") {
      obj.overload = message.overload;
    }
    if (message.valueInfo?.length) {
      obj.valueInfo = message.valueInfo.map((e) => ValueInfoProto.toJSON(e));
    }
    if (message.metadataProps?.length) {
      obj.metadataProps = message.metadataProps.map((e) => StringStringEntryProto.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FunctionProto>, I>>(base?: I): FunctionProto {
    return FunctionProto.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FunctionProto>, I>>(object: I): FunctionProto {
    const message = createBaseFunctionProto();
    message.name = object.name ?? "";
    message.input = object.input?.map((e) => e) || [];
    message.output = object.output?.map((e) => e) || [];
    message.attribute = object.attribute?.map((e) => e) || [];
    message.attributeProto = object.attributeProto?.map((e) => AttributeProto.fromPartial(e)) || [];
    message.node = object.node?.map((e) => NodeProto.fromPartial(e)) || [];
    message.docString = object.docString ?? "";
    message.opsetImport = object.opsetImport?.map((e) => OperatorSetIdProto.fromPartial(e)) || [];
    message.domain = object.domain ?? "";
    message.overload = object.overload ?? "";
    message.valueInfo = object.valueInfo?.map((e) => ValueInfoProto.fromPartial(e)) || [];
    message.metadataProps = object.metadataProps?.map((e) => StringStringEntryProto.fromPartial(e)) || [];
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  if ((globalThis as any).Buffer) {
    return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
  } else {
    const bin = globalThis.atob(b64);
    const arr = new Uint8Array(bin.length);
    for (let i = 0; i < bin.length; ++i) {
      arr[i] = bin.charCodeAt(i);
    }
    return arr;
  }
}

function base64FromBytes(arr: Uint8Array): string {
  if ((globalThis as any).Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin: string[] = [];
    arr.forEach((byte) => {
      bin.push(globalThis.String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function longToNumber(int64: { toString(): string }): number {
  const num = globalThis.Number(int64.toString());
  if (num > globalThis.Number.MAX_SAFE_INTEGER) {
    throw new globalThis.Error("Value is larger than Number.MAX_SAFE_INTEGER");
  }
  if (num < globalThis.Number.MIN_SAFE_INTEGER) {
    throw new globalThis.Error("Value is smaller than Number.MIN_SAFE_INTEGER");
  }
  return num;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}